{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46db7834-95d1-4f64-80d8-30e0bb3dae50",
   "metadata": {},
   "source": [
    "# Citation\n",
    "\n",
    "```\n",
    "@misc{ToD-calibrate-2024,\n",
    "  author = {Matthew Yucha},\n",
    "  title = {TextEvolve Calibrate},\n",
    "  year = {2024},\n",
    "  month = sep,\n",
    "  url = {https://github.com/theobjectivedad/research/blob/main/calibrate/calibrate.ipynb},\n",
    "  note = {Jupyter Notebook, Contact: theobjectivedad@gmail.com}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8b126-5133-48a4-a5da-731544d4e7d8",
   "metadata": {},
   "source": [
    "# TextEvolve Calibrate\n",
    "\n",
    "\n",
    "*Calibrate* is a semi-supervised automatic prompt optimization (APO) method where a large language model (LLM) is used to automatically improve the performance of a prompt given a calibration data set. Similar to [Evaluate](https://github.com/theobjectivedad/research/blob/main/evaluate/Evaluate%20Demo.ipynb).\n",
    "\n",
    "*Calibrate* is an enhancement to Prompt Optimization with Textual Gradients (ProTeGi) [[arXiv:2305.03495](https://arxiv.org/abs/2305.03495)]. From the paper, ProTeGi refers to a novel APO method that proposes using a gradient descent-style algorithm to automatically optimize an LLM prompt against a training data set. While ProTeGi demonstrated impressive accuracy gains of up to 31% across three benchmark tasks, including jailbreak detection, there two significant limitations:\n",
    "\n",
    "1. Classification only: ProTeGi is limited to optimizing classification tasks (ex: jailbreak attempt/no jailbreak attempt)\n",
    "2. Serial beam selection: ProTeGi's proposed beam selection methods, *UCB Bandits*, *UCB-E*, *Successive Rejects*, and *Successive Halving* need to be computed serially making them challenging to scale.\n",
    "3. On its own, ProTeGi implements a supervised optimization methodology and requires data labeling.\n",
    "\n",
    "*Calibrate* addresses these limitations by replacing *UCB Bandits* beam selection with a batch capable LLM-based method. Additionally, *Evaluate* is used as the metric function $m$ which allows assigning a continuous numeric score to candidates during the selection process. \n",
    "\n",
    "*Calibrate* and *Evaluate* are part of the larger TextEvolve service suite. TextEvolve is a collection of integrated generative AI services focused on the guided and automated reasoning over text-based data. One of the main goals of TextEvolve is to remove the most labor-intensive components of building AI systems including: human evaluation, prompt engineering, and data set labeling. \n",
    "\n",
    "```text\n",
    "    ┌────────────────────┐  ┌────────────────────┐\n",
    "    │                    │  │                    │\n",
    "    │      Calibrate     │  │      Innovate      │\n",
    "    │                    │  │                    │\n",
    "    └────────────────────┘  └────────────────────┘\n",
    "    ┌────────────────────────────────────────────┐\n",
    "    │                                            │\n",
    "    │                  Evaluate                  │\n",
    "    │                                            │\n",
    "    └────────────────────────────────────────────┘\n",
    "    ┌────────────────────────────────────────────┐\n",
    "    │                                            │\n",
    "    │                  Generate                  │\n",
    "    │                                            │\n",
    "    └────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "As shown in the diagram above, each TextEvolve service builds from the others.\n",
    "\n",
    "**Generate**: Automatically generates agent profiles for *Evaluate* given an input task.\n",
    "\n",
    "**Evaluate**: Leverages a multi-agent debate (MAD) methodology to generate a numeric score given an input and set of outputs.\n",
    "\n",
    "**Calibrate**: A semi-supervised automatic prompt optimization (APO) method where a large language model (LLM) is used to automatically improve the performance of a prompt given a calibration data set.\n",
    "\n",
    "**Innovate**: An unsupervised method of iteratively improving the quality of an input. Like *Calibrate*, it leverages *Evaluate* to compute scores and \"gradients\" to guide the improvement process.\n",
    "\n",
    "## Method\n",
    "\n",
    "In every round $r$, expanded prompt template candidates $\\mathcal{C}$ are computed by applying the  $\\operatorname{expand}$ function over all elements of our beam $\\mathcal{B}$. \n",
    "\n",
    "$$\n",
    "C = \\left\\{ \\operatorname{expand}\\left(t': b, \\mathcal{D}_{cal}\\right) \\mid b \\in \\mathcal{B} \\right\\}\n",
    "$$\n",
    "\n",
    "During the expansion process, a mini-batch of samples are randomly selected from $\\mathcal{D}_{cal}$ and used to batch generate a response from $\\operatorname{LLM}$: \n",
    "\n",
    "$$\n",
    "\\operatorname{LLM}\\left(\\text{generations}: \\text{minibatch} \\right)\n",
    "$$\n",
    "\n",
    "Next, \"gradients\" $\\mathcal{g}$ are computed by invoking *Evaluate* where $x$ is the current prompt template in $b$ and $\\mathcal{y}$ contains the generations from the previous step. The actual values in $\\mathcal{g}$ are natural language instructions on how $b$ could be changed to achieve a better score w.r.t. the template values sampled from $\\mathcal{D}_{cal}$ in the current mini-batch:\n",
    "\n",
    "$$\n",
    "\\operatorname{LLM}_{\\nabla}\\left(\\mathcal{g} : x, \\mathcal{y}\\right)\n",
    "$$\n",
    "\n",
    "The last step is where we will actually generate new prompt templates given our current beam $b$, and gradients $\\mathcal{g}$:\n",
    "\n",
    "$$\n",
    "\\operatorname{LLM}_{\\sigma}\\left(t'_b: b, \\mathcal{g} \\right)\n",
    "$$\n",
    "\n",
    "At this point expanded prompt template candidates $\\mathcal{C}$ have been derived and the *Calibrate* fast selection process is executed:\n",
    "\n",
    "$$\n",
    "\\operatorname{select}_{\\text{fast}}\\left( \\mathcal{B'}: \\mathcal{B}, \\mathcal{C}, \\mathcal{D}_{cal} \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\operatorname{select}_{\\text{fast}}$ will again sample a mini-batch from $\\mathcal{D}_{cal}$, generate outputs via $\\operatorname{LLM}$  and compute numeric scores via *Evaluate*. The implementation will execute batch inferencing operations on all LLM calls, including *Evaluate* to achieve scalability. New beams $\\mathcal{B}$ will be selected from the highest scoring prompt templates and the next round will begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b6ca4-f70e-4d98-a311-f3e93633d339",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "This simulation shows a demonstration calibration run.\n",
    "\n",
    "\n",
    "Suppose we have a simple prompt template $t_0$:\n",
    "\n",
    "```\n",
    "Write a {document} about {topic}\n",
    "```\n",
    "\n",
    "We would like to automatically calibrate this prompt template against the following set of data $D_{\\text{cal}}$, observe that each element contains natural language values for $t_0$: \n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"document\": \"report\", \"topic\": \"AI\"},\n",
    "    {\"document\": \"essay\", \"topic\": \"philosophy\"},\n",
    "    {\"document\": \"article\", \"topic\": \"technology\"},\n",
    "    {\"document\": \"post\", \"topic\": \"economics\"},\n",
    "    {\"document\": \"review\", \"topic\": \"politics\"},\n",
    "    {\"document\": \"paper\", \"topic\": \"history\"},\n",
    "    {\"document\": \"guide\", \"topic\": \"science\"},\n",
    "    {\"document\": \"letter\", \"topic\": \"art\"},\n",
    "    {\"document\": \"memo\", \"topic\": \"education\"},\n",
    "    {\"document\": \"summary\", \"topic\": \"health\"},\n",
    "    {\"document\": \"proposal\", \"topic\": \"business\"},\n",
    "    {\"document\": \"manual\", \"topic\": \"literature\"},\n",
    "    {\"document\": \"story\", \"topic\": \"environment\"},\n",
    "    {\"document\": \"script\", \"topic\": \"culture\"},\n",
    "    {\"document\": \"novel\", \"topic\": \"society\"},\n",
    "    {\"document\": \"blog\", \"topic\": \"media\"},\n",
    "    {\"document\": \"journal\", \"topic\": \"sports\"},\n",
    "    {\"document\": \"thesis\", \"topic\": \"entertainment\"},\n",
    "    {\"document\": \"dissertation\", \"topic\": \"food\"},\n",
    "    {\"document\": \"book\", \"topic\": \"travel\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Given $t_0$ and $\\mathcal{D}_{\\text{cal}}$, we initialize *Calibrate* with the following hyper-parameters:\n",
    "\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "| --------- | ----- | ----------- |\n",
    "| ``b`` | 4 | Beam width, controls how many beams to explore for each search level $r$ |\n",
    "| ``r`` | 5 | Search depth, controls how many search iterations are performed |\n",
    "| ``minibatch_exp_len`` | 5 | Number of calibration data samples used during expansion |\n",
    "| ``minibatch_sel_len`` | 6 | Number of calibration data samples for the scoring step during selection |\n",
    "| ``grad_depth`` | 10 | Number of suggestions the LLM generates when constructing gradients |\n",
    "| ``grad_debate_rounds`` | 1 | Number of *Evaluate* debate rounds to consider when generating gradients. |\n",
    "| ``mc_successors`` | 5 | Number of monte-carlo successors generated from the improved template. |\n",
    "\n",
    "Next, we execute the simulation which modifies *Evaluate* to always increments $\\mathcal{B}$ by $+0.200$ when $\\mathcal{B}_i=2$, and decrements $\\mathcal{B}$ by the same amount for all other beams: \n",
    "\n",
    "```text\n",
    "2024-09-19 13:41:21 INFO r=01 expand(B=00): len(t')=05, len(C)=05\n",
    "2024-09-19 13:41:21 INFO r=01 select[fast]: len(B')=4, new beams=4, len(B)=4, B mean=5.0375 (4.9833, 5.2000, 4.9833, 4.9833)\n",
    "\n",
    "2024-09-19 13:41:21 INFO r=02 expand(B=00): len(t')=05, len(C)=05\n",
    "2024-09-19 13:41:21 INFO r=02 expand(B=01): len(t')=05, len(C)=10\n",
    "2024-09-19 13:41:21 INFO r=02 expand(B=02): len(t')=05, len(C)=15\n",
    "2024-09-19 13:41:21 INFO r=02 expand(B=03): len(t')=05, len(C)=20\n",
    "2024-09-19 13:41:22 INFO r=02 select[fast]: len(B')=4, new beams=1, len(B)=4, B mean=5.0750 (4.9667, 5.4000, 4.9667, 4.9667)\n",
    "\n",
    "2024-09-19 13:41:22 INFO r=03 expand(B=00): len(t')=05, len(C)=05\n",
    "2024-09-19 13:41:23 INFO r=03 expand(B=01): len(t')=05, len(C)=10\n",
    "2024-09-19 13:41:23 INFO r=03 expand(B=02): len(t')=05, len(C)=15\n",
    "2024-09-19 13:41:23 INFO r=03 expand(B=03): len(t')=05, len(C)=20\n",
    "2024-09-19 13:41:24 INFO r=03 select[fast]: len(B')=4, new beams=1, len(B)=4, B mean=5.1125 (4.9500, 5.6000, 4.9500, 4.9500)\n",
    "\n",
    "2024-09-19 13:41:24 INFO r=04 expand(B=00): len(t')=05, len(C)=05\n",
    "2024-09-19 13:41:24 INFO r=04 expand(B=01): len(t')=05, len(C)=10\n",
    "2024-09-19 13:41:24 INFO r=04 expand(B=02): len(t')=05, len(C)=15\n",
    "2024-09-19 13:41:24 INFO r=04 expand(B=03): len(t')=05, len(C)=20\n",
    "2024-09-19 13:41:25 INFO r=04 select[fast]: len(B')=4, new beams=1, len(B)=4, B mean=5.1500 (4.9333, 5.8000, 4.9333, 4.9333)\n",
    "\n",
    "2024-09-19 13:41:25 INFO r=05 expand(B=00): len(t')=05, len(C)=05\n",
    "2024-09-19 13:41:25 INFO r=05 expand(B=01): len(t')=05, len(C)=10\n",
    "2024-09-19 13:41:25 INFO r=05 expand(B=02): len(t')=05, len(C)=15\n",
    "2024-09-19 13:41:26 INFO r=05 expand(B=03): len(t')=05, len(C)=20\n",
    "2024-09-19 13:41:26 INFO r=05 select[fast]: len(B')=4, new beams=1, len(B)=4, B mean=5.1875 (4.9167, 6.0000, 4.9167, 4.9167)\n",
    "```\n",
    "\n",
    "### Inferencing Volume\n",
    "\n",
    "This table aggregates LLM operation statistics from the simulation run.\n",
    "\n",
    "| LLM Operation                                    | Total | Success | Error | Request Tokens | Response Tokens | Total Tokens |\n",
    "| :----------------------------------------------- | ----: | ------: | ----: | -------------: | --------------: | -----------: |\n",
    "| $\\operatorname{E}$, Evaluate turn                |   612 |     612 |     0 |        1,089,352 |          516,764 |      1,606,116 |\n",
    "| $\\operatorname{LLM_{\\text{gen}, \\text{select}}}$ |   510 |     510 |     0 |         100,638 |          104,733 |       205,371 |\n",
    "| $\\operatorname{LLM}_{\\text{gen}, \\text{expand}}$ |    85 |      85 |     0 |          16,617 |           17,492 |        34,109 |\n",
    "| $\\operatorname{LLM}_{\\sigma}$                    |    85 |      85 |     0 |         215,315 |           17,100 |       232,415 |\n",
    "| $\\operatorname{LLM}_{\\nabla}$                    |    17 |      17 |     0 |           6,978 |            3,414 |        10,392 |\n",
    "\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This table aggregates estimated costs across OpenAI models from the simulation run. Prices are current as of 2024-08-17.\n",
    "\n",
    "| LLM Name   | Total Cost | Request Cost | Response Cost | Request Tokens | Response Tokens | Total Tokens | Generation Count |\n",
    "| ---------- | ---------- | ------------ | ------------- | -------------- | --------------- | ------------ | ---------------- |\n",
    "| gpt4o      | \\$10.17    | \\$3.57       | \\$6.60        | 1,428,900      | 659,503         | 2,088,403    | 1,309            |\n",
    "| gpt4o-mini | \\$0.61     | \\$0.21       | \\$0.40        | 1,428,900      | 659,503         | 2,088,403    | 1,309            |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ffed0-a361-49d8-8f22-14845fb40e49",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "1. From the ProTeGi paper, *UCB Bandits* was the best performing beam selection method and despite it's scalability shortcomings may improve the effectiveness of *Calibrate*.\n",
    "\n",
    "2. The fast selection method implemented by *Calibrate* could be improved significantly by testing more mini-batches of calibration data. Additional testing is needed to confirm the hypothesis that by scoring more calibration data during select, performance will be significantly improved (similar to a validation operation during deep neural network training). Additionally, with enough $\\mathcal{D}_{\\text{cal}}$ examples, there may be additional performance improvements realized by splitting $\\mathcal{D}_{\\text{cal}}$ into a calibration and validation data set to help prevent over-fitting prompt templates to calibration data that doesn't fully represent the overall task of the prompt.\n",
    "\n",
    "3. Automatic hyper-parameter optimization; this will be needed to make *Calibrate* available to the everyman.\n",
    "\n",
    "4. Additional research is needed to determine to what extent other beam selection methods such as *UCB Bandits* improve the overall performance of *Calibrate*.\n",
    "\n",
    "5. Large context support could be implemented via a map-reduce summarization (I think).\n",
    "\n",
    "6. While basic prompts were provided for $\\operatorname{LLM}_{\\sigma}$ and $\\operatorname{LLM}_{\\nabla}$, specific optimizations described in [[arXiv:2406.06608](https://arxiv.org/abs/2406.06608)] would likely improve performance.\n",
    "\n",
    "7. Integration with experiment tracking tools such as [WanDB.ai](https://wandb.ai/)\n",
    "\n",
    "8. Early stopping mechanics can be improved by checking a score $s$ tolerance, ex $\\overline{s} \\pm x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd309b-4266-43ed-9249-7fce8a02c647",
   "metadata": {},
   "source": [
    "## Appendix A: Code\n",
    "\n",
    "This section contains an executable demonstration of *Evaluate* and *Calibrate*.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "The following code blocks contain dependencies needed for this demo. These include a simple LLM simulation class as well as an implementation of [Evaluate](https://www.theobjectivedad.com/pub/20240827-textevolve-evaluate/index.html). Please execute these cells first before running the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfa881a-6dce-4c95-85e8-245582d70508",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"LLM Simulator for offline development and testing\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from decimal import ROUND_HALF_UP, Decimal\n",
    "from typing import Any, Dict, List, Literal, Tuple, Union\n",
    "\n",
    "import tiktoken\n",
    "from faker import Faker\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMSimulatorBase(ABC):\n",
    "    \"\"\"\n",
    "    Simulates calling a Language Model (LLM) by generating a list of simulated\n",
    "    responses from one or more rendered prompts.\n",
    "\n",
    "    This class is a very loosely based on langchain_core.langchain_models.llms.BaseLLM\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompts: Union[List[str], str],\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "        \"\"\"\n",
    "        Abstract method to generate simulated LLM responses.\n",
    "\n",
    "        Args:\n",
    "            prompts (Union[List[str], str]): Fully rendered prompt(s) that would be\n",
    "                passed to the LLM.\n",
    "            n (int, optional): Number of responses to generate for each prompt. Defaults\n",
    "                to 1.\n",
    "            prefix (str, optional): Prefix to add to each generated response. Defaults\n",
    "                to \"GEN\".\n",
    "            append (str, optional): String to append at the end of each generated\n",
    "                response. Defaults to None.\n",
    "            min_tokens (int, optional): Minimum number of tokens in each simulated LLM\n",
    "                response. Defaults to None.\n",
    "            max_tokens (int, optional): Maximum number of tokens in each simulated LLM\n",
    "                response. Defaults to None.\n",
    "            **kwargs: Additional arguments, intended use is to document / capture\n",
    "                additional model parameters passed to inferencing function\n",
    "\n",
    "        Returns:\n",
    "            List[str | None]: A list of generated responses, each corresponding to a\n",
    "                prompt. When a respnse contains None, it represents a LLM inferencing\n",
    "                error.\n",
    "        \"\"\"\n",
    "\n",
    "    def invoke(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "        return self(\n",
    "            prompt,\n",
    "            n=n,\n",
    "            prefix=prefix,\n",
    "            append=append,\n",
    "            min_tokens=min_tokens,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    async def ainvoke(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "        return self(\n",
    "            prompt,\n",
    "            n=n,\n",
    "            prefix=prefix,\n",
    "            append=append,\n",
    "            min_tokens=min_tokens,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "        return self(\n",
    "            prompts,\n",
    "            n=n,\n",
    "            prefix=prefix,\n",
    "            append=append,\n",
    "            min_tokens=min_tokens,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    async def abatch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "        return self(\n",
    "            prompts,\n",
    "            n=n,\n",
    "            prefix=prefix,\n",
    "            append=append,\n",
    "            min_tokens=min_tokens,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def render_template(\n",
    "        *, template: str, template_values: Union[List[Dict[str, Any]], Dict[str, Any]]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Substitutes placeholders in a template string with values from a dictionary or\n",
    "        a list of dictionaries.\n",
    "\n",
    "        Args:\n",
    "            template (str): The template string containing placeholders in the format\n",
    "                '{key}'.\n",
    "            template_values (Union[List[Dict[str, Any]], Dict[str, Any]]): A dictionary\n",
    "                or list of dictionaries where keys match the placeholders in the\n",
    "                template, and values are the substitutions.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of rendered template strings with placeholders substituted\n",
    "            by their corresponding values.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If a substitution pattern in 'template' is not found in\n",
    "                'template_values', or if a key in 'template_values' is not found in\n",
    "                'template'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate and normalize the template_values input\n",
    "        if isinstance(template_values, Dict):\n",
    "            template_values = [template_values]\n",
    "\n",
    "        # Extract all substitution patterns from the template (placeholders in the form '{key}')\n",
    "        template_keys = set(re.findall(r\"{(.*?)}\", template))\n",
    "        rendered_templates = []\n",
    "\n",
    "        for v_index, cur_template_value in enumerate(template_values):\n",
    "            # Check for missing template keys in the current value dictionary\n",
    "            missing_keys = template_keys - set(cur_template_value.keys())\n",
    "            if missing_keys:\n",
    "                raise ValueError(\n",
    "                    f\"Missing keys {missing_keys} in template_values at index {v_index}\"\n",
    "                )\n",
    "\n",
    "            # Check for extra keys in the current value dictionary that are not used in the template\n",
    "            extra_keys = set(cur_template_value.keys()) - template_keys\n",
    "            if extra_keys:\n",
    "                raise ValueError(\n",
    "                    f\"Extra keys {extra_keys} in template_values at index {v_index} not found in template\"\n",
    "                )\n",
    "\n",
    "            # Render the template by replacing placeholders with their corresponding values\n",
    "            rendered_template = template\n",
    "            for key, value in cur_template_value.items():\n",
    "                placeholder = f\"{{{key}}}\"\n",
    "                rendered_template = rendered_template.replace(placeholder, str(value))\n",
    "\n",
    "            rendered_templates.append(rendered_template.strip())\n",
    "\n",
    "        return rendered_templates\n",
    "\n",
    "\n",
    "class LLMSimulatorLite(LLMSimulatorBase):\n",
    "    \"\"\"\n",
    "    Simple LLM simulator with minimal external dependencies that generates a fixed\n",
    "    response for each prompt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique: bool = True, chars_per_token: int = 3):\n",
    "\n",
    "        # When true, add a unique element to each prompt. This is useful when testing\n",
    "        # applications that use set-style collections as part of their algorithm.\n",
    "        self.unique = unique\n",
    "\n",
    "        # When constructing responses of a desired length, this is the calue that is\n",
    "        # used to estimate the number of tokens per character.\n",
    "        self.chars_per_token = chars_per_token\n",
    "\n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        return math.ceil(len(text) / self.chars_per_token)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompts: Union[List[str], str],\n",
    "        *,\n",
    "        n: int = 1,\n",
    "        prefix: str = \"GEN\",\n",
    "        append: str | None = None,\n",
    "        min_tokens: int | None = None,\n",
    "        max_tokens: int | None = None,\n",
    "        **kwargs,\n",
    "    ) -> List[str | None]:\n",
    "\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "\n",
    "        # Generate a fixed response for each prompt, assume 3 characters per token\n",
    "        if min_tokens is not None and max_tokens is not None:\n",
    "            target_tokens = random.randint(min_tokens, max_tokens)\n",
    "        else:\n",
    "            target_tokens = 100\n",
    "\n",
    "        derived_append = append if append is not None else \"\"\n",
    "\n",
    "        prefix_tokens = self._estimate_tokens(prefix)\n",
    "        derived_append_tokens = self._estimate_tokens(derived_append)\n",
    "\n",
    "        # Generate response strings for each prompt response value\n",
    "        responses: List[str | None] = []\n",
    "        for _ in range(n * len(prompts)):\n",
    "\n",
    "            # Generate a unique UUID for each response if required\n",
    "            if self.unique:\n",
    "                derived_uuid = str(uuid4()).replace(\"-\", \"\")\n",
    "            else:\n",
    "                derived_uuid = \"\"\n",
    "\n",
    "            # Estimate the number of tokens for each part of the response\n",
    "            uuid_tokens = self._estimate_tokens(derived_uuid)\n",
    "\n",
    "            # Calculate how many filler tokens we need to generate\n",
    "            filler_tokens = target_tokens - (\n",
    "                uuid_tokens + prefix_tokens + derived_append_tokens\n",
    "            )\n",
    "\n",
    "            # Calculate how many filler characters we need to generate, note the -3 is\n",
    "            # estimating for the shitespace in between response template fields below.\n",
    "            filler_chars = math.ceil(filler_tokens * self.chars_per_token) - 3\n",
    "\n",
    "            filler = \"A\" * filler_chars\n",
    "\n",
    "            cur_response = f\"{prefix},{derived_uuid},{filler},{derived_append}\".strip()\n",
    "            responses.append(cur_response)\n",
    "\n",
    "        return responses\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fa840e-d212-4e53-a285-d1a5c0f2bac2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate MK2\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from sandbox.llm_simulator import LLMSimulatorBase, LLMSimulatorLite\n",
    "\n",
    "\n",
    "class Evaluate:\n",
    "    \"\"\"\n",
    "    MK2 sandbox implementation of the TextEvolve Evaluate function.\n",
    "\n",
    "    Technical paper: https://bit.ly/3MoNl7A\n",
    "\n",
    "    Note:\n",
    "        Changes from MK1\n",
    "\n",
    "        * Fixed a bug in the normalized score calculation\n",
    "        * Added parallelized batch evaluation support\n",
    "        * Added simulated memory selection method\n",
    "        * Added debate history selection\n",
    "        * Integrated the LLMSimulator for generating debate turn prompts\n",
    "        * Performed code quality checks and improvements\n",
    "        * Added minor documentation improvements\n",
    "        * Added implementation notes where appropriate\n",
    "        * Removed pointless debugging print statements\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        a: List[str],\n",
    "        w: np.ndarray,\n",
    "        r: int,\n",
    "        c: float,\n",
    "        l: int,  # noqa: E741\n",
    "        llm_sim: LLMSimulatorBase | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the given configuration settings.\n",
    "\n",
    "        Args:\n",
    "            a (List[str]): List of debater agents.\n",
    "            w (np.ndarray): Weight vector for score components.\n",
    "            r (int): Number of debate rounds.\n",
    "            c (float): Convergence threshold for early stopping.\n",
    "            l (int): Debate history parameter (currently a placeholder).\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.w = w\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.l = l  # noqa: E741\n",
    "\n",
    "        # Provide an instance of LLMSimulator, this is used to simulate calling the LLM\n",
    "        # for demonstrations, unit testing, debugging\n",
    "        self.llm_sim = llm_sim if llm_sim is not None else LLMSimulatorLite()\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def _debate_turn(\n",
    "        self,\n",
    "        x: str,\n",
    "        y: List[str],\n",
    "        xi: List[str],\n",
    "        m_i: List[str],\n",
    "        i: int,\n",
    "        j: int,\n",
    "        k: int,\n",
    "        round_num: int,\n",
    "        x_name: str = \"input\",\n",
    "        y_name: str = \"output\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulate the LLM scoring process for a debate turn using a truncated normal\n",
    "        distribution. This simulation controls the coefficient of variation (CV) to\n",
    "        mimic agents reaching consensus over time.\n",
    "\n",
    "        Note:\n",
    "            Actual implementations of this function should check to ensure the\n",
    "            score values returned from the LLM fall within the 0.0 to 10.0 range.\n",
    "            Problematic responses can either be retried, dropped, or a scaling heuristic\n",
    "            can be applied to bring them into the valid range.\n",
    "\n",
    "        Note:\n",
    "            Actual implementation should consider error handling and retey logic for\n",
    "            failures, in particular parse errors or scores that are out of bounds.\n",
    "\n",
    "        Args:\n",
    "            x (str): The input context in natural language.\n",
    "            y (List[str]): List of candidate responses.\n",
    "            xi (List[str]): Debate history.\n",
    "            m_i (List[str]): Memories for the current agent.\n",
    "            i (int): Agent identifier.\n",
    "            j (int): Number of candidate responses.\n",
    "            k (int): Number of score components.\n",
    "            round_num (int): The current round number.\n",
    "            x_name (str): Name of the input context, used to build the debate turn\n",
    "            prompt\n",
    "            y_name (str): Name of the candidate responses, used to build the debate turn\n",
    "            prompt.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A j x k matrix with scores between 0.0 and 10.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample debate turn prompt, this will likely need to be optimized for your use\n",
    "        # case. The instructions are tuned to be flexible enough to evaluate a broad\n",
    "        # range of x and y variations and agent personas, while pinning the agent to the\n",
    "        # evaluation criteria to be externally weighted by confidence, relevancy,\n",
    "        # accuracy, completness, timeliness, and overall score components.\n",
    "        prompt_template = \"\\n\".join(\n",
    "            [\n",
    "                \"Your are the {role}. {role_description}\",\n",
    "                \"\",\n",
    "                \"You are participating in a multi-agent debate to evaluate the {x_name} against each {y_name} below.\",\n",
    "                \"\",\n",
    "                \"----- BEGIN {x_name_upper} -----\",\n",
    "                \"{x}\",\n",
    "                \"----- END {x_name_upper} -----\",\n",
    "                \"\",\n",
    "                \"----- BEGIN {y_name_upper} TO EVALUATE -----\",\n",
    "                \"{y}\",\n",
    "                \"----- END {y_name_upper} TO EVALUATE -----\",\n",
    "                \"\",\n",
    "                \"----- BEGIN DEBATE HISTORY -----\",\n",
    "                \"{xi}\",\n",
    "                \"----- END DEBATE HISTORY -----\",\n",
    "                \"\",\n",
    "                \"As the {role}, you recall the following memories relevant to this debate:\",\n",
    "                \"\",\n",
    "                \"----- BEGIN {role_upper} MEMORIES -----\",\n",
    "                \"{m_i}\",\n",
    "                \"----- END {role_upper} MEMORIES -----\",\n",
    "                \"\",\n",
    "                \"Your tasks as the {role}, is to evaluate each {y_name} and contribute to the debate.\",\n",
    "                \"Consider the following for each {y_name}:\",\n",
    "                \"1. As needed, address previous points raised in the debate as they pertain to your evaluation\",\n",
    "                \"2. Analyze the relevance and accuracy for each {y_name}\",\n",
    "                \"3. Analyze the completeness and depth for each {y_name}\",\n",
    "                \"4. Any potential limitations for each {y_name}\",\n",
    "                \"5. Analyze any potential limitations for each {y_name}\",\n",
    "                \"6. Analyze any timeliness considerations for each {y_name}, is it currently: {current_time}\",\n",
    "                \"7. Missing information and/or context for each {y_name}\",\n",
    "                \"\",\n",
    "                \"Provide your evaluation in a clear, to-the-point, and concise manner, staying true to your role.\",\n",
    "                \"\",\n",
    "                \"You must evaluate all {y_count} {y_name}.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        agent_profiles: Dict[str, str] = {\n",
    "            \"Critic\": \"\"\n",
    "            \"You are a critical thinker who analyzes responses for flaws and \"\n",
    "            \"inconsistencies. You question assumptions, point out logical \"\n",
    "            \"fallacies, and identify areas where the responses may be lacking or \"\n",
    "            \"misleading.\",\n",
    "            \"Supporter\": \"\"\n",
    "            \"You are an advocate who looks for strengths and positive aspects in the \"\n",
    "            \"responses. Your highlight the merits of each response, explain \"\n",
    "            \"potential benefits, and defend good ideas against criticism.\",\n",
    "            \"Neutral Observer\": \"\"\n",
    "            \"You are an impartial observer who balances different viewpoints and \"\n",
    "            \"provides objective analysis. You consider all perspectives, \"\n",
    "            \"weigh the pros and cons, and offer a balanced evaluation of the \"\n",
    "            \"responses.\",\n",
    "        }\n",
    "\n",
    "        # For the simulation, randomly select an agent role\n",
    "        role, role_description = random.choice(list(agent_profiles.items()))\n",
    "\n",
    "        rendered_prompt = self.llm_sim.render_template(\n",
    "            template=prompt_template,\n",
    "            template_values={\n",
    "                \"role\": role,\n",
    "                \"role_upper\": role.upper(),\n",
    "                \"role_description\": role_description,\n",
    "                \"x\": x,\n",
    "                \"x_name\": x_name,\n",
    "                \"x_name_upper\": x_name.upper(),\n",
    "                \"y\": \"\\n\\n\".join(\n",
    "                    [\n",
    "                        f\"{y_name} {y_index+1}:\\n{y_value}\"\n",
    "                        for y_index, y_value in enumerate(y)\n",
    "                    ]\n",
    "                ),\n",
    "                \"y_name\": y_name,\n",
    "                \"y_name_upper\": y_name.upper(),\n",
    "                \"y_count\": len(y),\n",
    "                \"xi\": (\n",
    "                    \"\\n\".join(xi) if len(xi) > 0 else \"(No debate history, first round)\"\n",
    "                ),\n",
    "                \"m_i\": (\n",
    "                    \"\\n\".join(m_i)\n",
    "                    if len(m_i) > 0\n",
    "                    else \"(No external memories recalled)\"\n",
    "                ),\n",
    "                \"current_time\": datetime.now(timezone.utc).strftime(\n",
    "                    \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.llm_sim(\n",
    "            rendered_prompt,\n",
    "            n=1,\n",
    "            prefix=\"EVALUATE_TURN\",\n",
    "            min_tokens=200,\n",
    "            max_tokens=1500,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        # Simulation configuration: The Initial coefficient of variation for the first\n",
    "        # round.\n",
    "        initial_cv: float = 0.2\n",
    "\n",
    "        # Simulation configuration: Percentage by which CV decreases each round.\n",
    "        cv_decrease_percent: float = 10.0\n",
    "\n",
    "        # Generate mean scores for each candidate response and score component\n",
    "        # The mean is randomly chosen between 4.5 and 5.5 to center the scores around\n",
    "        # the middle of the 0-10 range\n",
    "        mean_score = np.random.uniform(4.5, 6.5, (j, k))  # Shape: (j, k)\n",
    "\n",
    "        # Compute the current coefficient of variation (CV) for this round\n",
    "        # CV decreases with each round by the specified percentage to simulate agents\n",
    "        # reaching consensus\n",
    "        current_cv = initial_cv * (1 - cv_decrease_percent / 100.0) ** round_num\n",
    "\n",
    "        # Standard deviation is calculated as a proportion of the mean score, based on\n",
    "        # the current CV\n",
    "        std_dev = mean_score * current_cv  # Shape: (j, k)\n",
    "\n",
    "        # Set the lower and upper bounds for the scores to ensure they stay within the\n",
    "        # valid range [0.0, 10.0]\n",
    "        lower, upper = 0.0, 10.0\n",
    "\n",
    "        # Generate scores using a truncated normal distribution The scores are generated\n",
    "        # such that they fall within the [0.0, 10.0] range, following a normal\n",
    "        # distribution centered on mean_score with std_dev\n",
    "        scores = truncnorm(\n",
    "            (lower - mean_score) / std_dev,  # Lower bound in standardized units\n",
    "            (upper - mean_score) / std_dev,  # Upper bound in standardized units\n",
    "            loc=mean_score,  # Mean of the distribution\n",
    "            scale=std_dev,  # Standard deviation of the distribution\n",
    "        ).rvs()  # Generate random variates\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_cv(matrix: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the coefficient of variation (CV) for a given score matrix.\n",
    "\n",
    "        Args:\n",
    "            matrix (np.ndarray): A matrix of scores (i x j x k) from which CV is\n",
    "            computed.\n",
    "\n",
    "        Returns:\n",
    "            float: The coefficient of variation of the scores.\n",
    "        \"\"\"\n",
    "        return np.std(matrix) / np.mean(matrix)\n",
    "\n",
    "    def evaluate_batch(\n",
    "        self,\n",
    "        batch: List[Tuple[str, List[str]]],\n",
    "        x_name: str = \"input\",\n",
    "        y_name: str = \"output\",\n",
    "        workers: int = 10,\n",
    "        thread_name_prefix: str = \"evaluate_pool\",\n",
    "    ) -> List[Tuple[np.ndarray, List[str]] | None]:\n",
    "        \"\"\"\n",
    "        Evaluates a batch of records in parallel using multiple worker threads.\n",
    "\n",
    "        This method will always preserve the order of the input batch when returning.\n",
    "\n",
    "        Args:\n",
    "            batch (List[Tuple[str, List[str]]]): The batch of records to evaluate. Each\n",
    "                record is a tuple containing an input string and a list of strings. workers\n",
    "                (int, optional): The number of worker threads to use. Defaults to 10.\n",
    "            thread_name_prefix (str, optional): The prefix for the worker thread names.\n",
    "                Defaults to \"evaluate_pool\".\n",
    "            workers (int, optional): The number of worker threads to use. Defaults to 10.\n",
    "            x_name (str): Name of the input context, used to build the debate turn\n",
    "                prompt.\n",
    "            y_name (str): Name of the candidate responses, used to build the debate turn\n",
    "                prompt.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, List[str]] | None]: A list of tuples containing the\n",
    "            evaluation result (S) and the debate for each record in the batch. If an\n",
    "            error occurs during evaluation, the tuple will contain None values.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "\n",
    "        Examples:\n",
    "            # Example usage\n",
    "            batch = [\n",
    "                (\"input1\", [\"string1\", \"string2\"]),\n",
    "                (\"input2\", [\"string3\", \"string4\"]),\n",
    "                (\"input3\", [\"string5\", \"string6\"]),\n",
    "            ]\n",
    "            results = evaluate_batch(batch, workers=5)\n",
    "\n",
    "        \"\"\"\n",
    "        pool = ThreadPoolExecutor(\n",
    "            thread_name_prefix=thread_name_prefix, max_workers=workers\n",
    "        )\n",
    "\n",
    "        def worker(\n",
    "            key: int,\n",
    "            x: str,\n",
    "            y: List[str],\n",
    "            x_name: str,\n",
    "            y_name: str,\n",
    "        ) -> Tuple[int, np.ndarray, List[str]] | Tuple[int, None, None]:\n",
    "            \"\"\"\n",
    "            Worker thread inner function that evaluates a batch record.\n",
    "\n",
    "            Args:\n",
    "                key (int): The key for the batch record.\n",
    "                x (str): The input string.\n",
    "                y (List[str]): The list of strings.\n",
    "\n",
    "\n",
    "            Returns:\n",
    "                Tuple[int, np.ndarray, List[str]] | Tuple[int, None, None]: A tuple\n",
    "                containing the key, the evaluation result (S), and the debate. If an\n",
    "                error occurs during evaluation, returns a tuple with None values.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                S, debate = self.evaluate(x=x, y=y, x_name=x_name, y_name=y_name)\n",
    "                return (key, S, debate)\n",
    "            except:\n",
    "\n",
    "                LOG.exception(\"Error evaluating batch record: %i\", key)\n",
    "                return (key, None, None)\n",
    "\n",
    "        try:\n",
    "            # Submit worker pool tasks\n",
    "            futures = []\n",
    "            for i, batch_record in enumerate(batch):\n",
    "                x, y = batch_record\n",
    "                future = pool.submit(worker, i, x, y, x_name, y_name)\n",
    "                futures.append(future)\n",
    "\n",
    "            # Get results in the order of the batch\n",
    "            keyed_results = []\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                keyed_results.append(result)\n",
    "\n",
    "            # Sort the results based on the original order of the batch\n",
    "            keyed_results = sorted(keyed_results, key=lambda x: x[0])\n",
    "\n",
    "            # Remove key from results since it was only needed to maintain order of the\n",
    "            # final result.\n",
    "            eval_results: List[Tuple[np.ndarray, List[str]] | None] = [None] * len(\n",
    "                batch\n",
    "            )\n",
    "            for cur_keyed_result in keyed_results:\n",
    "\n",
    "                key = cur_keyed_result[0]\n",
    "                S = cur_keyed_result[1]\n",
    "                debate = cur_keyed_result[2]\n",
    "\n",
    "                # Skip 'None' values, these are errors\n",
    "                if S is None or debate is None:\n",
    "                    continue\n",
    "\n",
    "                # Else, store the result to be returned\n",
    "                eval_results[key] = (\n",
    "                    S,\n",
    "                    debate,\n",
    "                )\n",
    "\n",
    "            return eval_results\n",
    "        finally:\n",
    "            pool.shutdown(wait=False, cancel_futures=True)\n",
    "\n",
    "    def _m(self, i: int, x: str, y: List[str], xi: List[str]) -> List[str]:\n",
    "        \"\"\"Placeholder method that simulates retrieving memories from an agent.\n",
    "\n",
    "        Args:\n",
    "            i (int): The agent index.\n",
    "            x (str): Input context in natural language.\n",
    "            y (List[str]): List of candidate responses.\n",
    "            xi (List[str]): Debate history, use by the memory retrievela algorithm.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of retrieved memories for the agent.\n",
    "        \"\"\"\n",
    "\n",
    "        return [f\"Memory {x+1}\" for x in range(10)]\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        x: str,\n",
    "        y: List[str],\n",
    "        x_name: str = \"input\",\n",
    "        y_name: str = \"output\",\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Evaluate the candidate responses using the TextEvolve evaluation function.\n",
    "\n",
    "        Args:\n",
    "            x (str): Input context in natural language.\n",
    "            y (List[str]): List of candidate responses.\n",
    "            x_name (str): Name of the input context, used to build the debate turn\n",
    "            prompt.\n",
    "            y_name (str): Name of the candidate responses, used to build the debate turn\n",
    "            prompt.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, List[str]]: Tuple containing the scoring tensor S with\n",
    "            dimensions (r x i x j x k) and simulated debate history.\n",
    "        \"\"\"\n",
    "        j = len(y)  # Number of candidate responses\n",
    "        k = len(self.w)  # Number of score components\n",
    "        i = len(self.a)  # Number of agents\n",
    "        S = np.zeros((self.r, i, j, k))  # Initialize scoring tensor\n",
    "\n",
    "        debate: List[str] = []\n",
    "\n",
    "        for round_num in range(self.r):\n",
    "            for agent_num in range(i):\n",
    "\n",
    "                # Placeholder for debate history (ξ)\n",
    "                xi = debate[-self.l * round_num :]\n",
    "\n",
    "                # Retrieve memories for the current agent\n",
    "                m_i = self._m(i=agent_num, x=x, y=y, xi=xi)\n",
    "\n",
    "                S[round_num, agent_num] = self._debate_turn(\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    xi=xi,\n",
    "                    m_i=m_i,\n",
    "                    i=agent_num,\n",
    "                    j=j,\n",
    "                    k=k,\n",
    "                    round_num=round_num,\n",
    "                    x_name=x_name,\n",
    "                    y_name=y_name,\n",
    "                )\n",
    "\n",
    "                # Simulate a debate history entry for this turn\n",
    "                ts = int(datetime.now().timestamp() * 1000) % 100000\n",
    "                debate.append(\n",
    "                    f\"{ts}: round {round_num}, agent {agent_num}, response candidates \"\n",
    "                    f\"1 to {j} {{DEBATE_TURN}}\"\n",
    "                )\n",
    "\n",
    "            # After each round, compute CV and check for early stopping\n",
    "            round_cv = self._compute_cv(matrix=S[round_num])\n",
    "\n",
    "            if round_cv <= self.c:\n",
    "                LOG.info(\n",
    "                    \"Early stopping triggered at Round %i (CV <= %f)\",\n",
    "                    round_num + 1,\n",
    "                    self.c,\n",
    "                )\n",
    "\n",
    "                S = S[: round_num + 1]  # Truncate the tensor to the completed rounds\n",
    "                break\n",
    "\n",
    "        return (S, debate)\n",
    "\n",
    "    def compute_normalized_scores(self, S: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the normalized scores for each response candidate.\n",
    "\n",
    "        Args:\n",
    "            S (np.ndarray): The scoring tensor with dimensions (r x i x j x k).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Normalized score vector for each candidate response.\n",
    "        \"\"\"\n",
    "        return np.sum(S * self.w, axis=(0, 1, 3)) / (\n",
    "            S.shape[0] * S.shape[1] * S.shape[3]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_softmax_scores(s_norm: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the softmax scores for each response candidate.\n",
    "\n",
    "        Args:\n",
    "            s_norm (np.ndarray): Normalized score vector.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Softmax score vector representing probabilities.\n",
    "        \"\"\"\n",
    "        e_x = np.exp(s_norm - np.max(s_norm))  # Subtract max for numerical stability\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def select_best_candidate(\n",
    "        self, S: np.ndarray, y: List[str], probabilistic: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Select the best response candidate based on normalized or softmax scores.\n",
    "\n",
    "        Args:\n",
    "            S (np.ndarray): The scoring tensor with dimensions (r x i x j x k).\n",
    "            y (List[str]): List of candidate responses.\n",
    "            probabilistic (bool): If True, selects based on softmax scores; otherwise,\n",
    "            based on normalized scores.\n",
    "\n",
    "        Returns:\n",
    "            str: The selected response candidate.\n",
    "        \"\"\"\n",
    "        # Compute normalized scores\n",
    "        s_norm = self.compute_normalized_scores(S)\n",
    "\n",
    "        if probabilistic:\n",
    "            # Compute softmax scores\n",
    "            s_phi = self.compute_softmax_scores(s_norm)\n",
    "\n",
    "            # Select the best response candidate probabilistically using softmax scores\n",
    "            best_index = np.random.choice(np.arange(len(y)), p=s_phi)\n",
    "        else:\n",
    "\n",
    "            # Select the best response candidate using normalized scores\n",
    "            best_index = np.argmax(s_norm)\n",
    "\n",
    "        return y[best_index]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd3be7-37dd-4c91-b509-5842b479c524",
   "metadata": {},
   "source": [
    "### Calibrate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b7a95b-d892-4a45-8368-1d2345ba1af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sandbox implementation of the TextEvolve Calibrate function.\"\"\"\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from typing import Any, Dict, List, Literal, Set, Tuple, cast\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sandbox.llm_simulator import LLMSimulatorBase, LLMSimulatorLite\n",
    "from sandbox.text_evolve.evaluate import Evaluate\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CalibrateException(Exception):\n",
    "    \"\"\"\n",
    "    TextEvolve Calibrate error.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Calibrate:\n",
    "    \"\"\"\n",
    "    TextEvolve Calibrate\n",
    "\n",
    "    This is a demonstration implementation of the TextEvolve Calibrate algorithm where\n",
    "    several sections of the code are stubbed out for simulation purposes. The algorithm\n",
    "    is designed to optimize a prompt template for a language model by iteratively\n",
    "    expanding and scoring the prompt template against a calibration dataset.\n",
    "\n",
    "    Production implementations will need to modify the following items:\n",
    "\n",
    "    * Remove the llm_sim parameter from the constructor\n",
    "    * Implement _llm_generate, _llm_gradients, and _llm_sigma, to invoke an actual LLM\n",
    "\n",
    "\n",
    "    Requires Python 3.11+\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        b: int,\n",
    "        r: int,\n",
    "        minibatch_exp_len: int,\n",
    "        minibatch_sel_len: int,\n",
    "        evaluator: Evaluate,\n",
    "        grad_depth: int = 10,\n",
    "        grad_debate_rounds: int = 2,\n",
    "        mc_successors: int = 10,\n",
    "        select_method: Literal[\"fast\", \"ucb\"] = \"fast\",\n",
    "        early_stopping: bool = True,\n",
    "        llm_sim: LLMSimulatorBase | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the TextEvolve Calibrate optimizer with the specified\n",
    "        hyper-parameters.\n",
    "\n",
    "        Args:\n",
    "            b (int): Beam width, determining how many candidate prompts to keep at each\n",
    "                level.\n",
    "            r (int): Search depth, or how many rounds of optimization to perform.\n",
    "            minibatch_exp_len (int): Number of calibration data samples for prompt\n",
    "                expansion.\n",
    "            minibatch_sel_len (int): Number of calibration data samples for scoring\n",
    "                expanded prompts.\n",
    "            evaluator (Evaluate): An instance of Evaluate used to evaluate and score\n",
    "                candidates.\n",
    "            grad_depth (int): Number of suggestions the LLM generates when constructing\n",
    "                gradients.\n",
    "            grad_debate_rounds (int): Number of debate rounds to consider when\n",
    "                generating gradients.\n",
    "            mc_successors (int): Number of Monte Carlo successors generated from the\n",
    "                improved template.\n",
    "            select_method (Literal[\"fast\", \"ucb\"]): Method for selecting candidates\n",
    "                ('fast' or 'ucb').\n",
    "            early_stopping (bool): If True, stops early if no new beams are discovered\n",
    "                in a round.\n",
    "            llm_sim (Optional[LLMSimulatorBase]): Instance of LLMSimulator for\n",
    "                simulation or testing.\n",
    "        \"\"\"\n",
    "\n",
    "        self.b = b  # Beam width\n",
    "        self.r = r  # Search depth\n",
    "\n",
    "        # Controls how many calibration data records are sampled for each prompt\n",
    "        # expansion. This value will vary depending on the backend LLM that is used\n",
    "        # where more sophisticated LLMs with larger token limits can handle larger\n",
    "        # mini-batches. Notice that large mini-batches will require more tokens and more\n",
    "        # sophisticated reasoning during scoring.\n",
    "        #\n",
    "        # Limitations: Too large a value will burden Evaluate with too many records to score\n",
    "        self.minibatch_exp_len = minibatch_exp_len\n",
    "\n",
    "        # Controls how many calibration records are samples when deriving the scores for\n",
    "        # expanded prompts\n",
    "        #\n",
    "        # Limitations: Too large a value will burden Evaluate with too many records to score\n",
    "        self.minibatch_sel_len = minibatch_sel_len\n",
    "\n",
    "        # Controls the number of suggestions the LLM is asked to generate when\n",
    "        # constructing gradients. More suggestions will allow for more sophisticated\n",
    "        # improvements per beam search iteration.\n",
    "        self.grad_depth = grad_depth\n",
    "\n",
    "        # Controls the number of debate rounds to consider when generating gradients. A\n",
    "        # round is a full debate cycle where each agent takes a turn. For example, if\n",
    "        # there are 3 agents, 1 debate round would include a response from all 3 agents.\n",
    "        self.grad_debate_rounds = grad_debate_rounds\n",
    "\n",
    "        # Once a new prompt template has been generated from gradients, this value\n",
    "        # controls how many additional monte-carlo successors will be generated from the\n",
    "        # improved template.\n",
    "        self.mc_successors = mc_successors\n",
    "\n",
    "        # A pre-configured instance if the TextEvolveEvaluator class\n",
    "        self.evaluator = evaluator\n",
    "\n",
    "        # Select method\n",
    "        self.select_method = select_method\n",
    "\n",
    "        # When early stopping is enabled, the number of new beams discovered will be\n",
    "        # checked after each round. If no new beams are detected the remaining rounds\n",
    "        # will be skipped.\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "        # Provide an instance of LLMSimulator, this is used to simulate calling the LLM\n",
    "        # for demonstrations, unit testing, debugging\n",
    "        self.llm_sim = llm_sim if llm_sim is not None else LLMSimulatorLite()\n",
    "\n",
    "    @staticmethod\n",
    "    def _top_k_beams(*, beams: Dict[str, float], k: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Selects the highest scoring beams.\n",
    "\n",
    "        Args:\n",
    "            beams (Dict[str, float]): A dictionary containing the beam text (key) and\n",
    "                score (value).\n",
    "            k (int): The number of highest scoring beams to select.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing the top k key-value pairs.\n",
    "        \"\"\"\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be positive\")\n",
    "\n",
    "        # If k is greater than or equal to the length of data, just return everything.\n",
    "        if k >= len(beams):\n",
    "            return beams\n",
    "\n",
    "        # Sort beams by score and select the last k items, these represent the highest\n",
    "        # scores\n",
    "        top_k_keys: List[str] = sorted(beams, key=beams.get)[-k:]  # type: ignore\n",
    "\n",
    "        # Create a new dictionary with the top k items\n",
    "        selected: Dict[str, float] = {}\n",
    "        for cur_k in top_k_keys:\n",
    "            selected[cur_k] = beams[cur_k]\n",
    "\n",
    "        return selected\n",
    "\n",
    "    def _llm_generate(\n",
    "        self,\n",
    "        templates: List[Dict[str, List[Dict[str, Any]]]],\n",
    "        prefix: str,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simulates calling an LLM for a batch of test generation templates.\n",
    "\n",
    "        Args:\n",
    "            templates (List[Dict[str, List[Dict[str, Any]]]]): A list where each item is a\n",
    "                dictionary mapping a prompt template (str) to a list of template values\n",
    "                (List[Dict[str, Any]]). Each template will be rendered with its corresponding\n",
    "                template values.\n",
    "            prefix (str): A prefix string used in the LLM simulation.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of generated strings from the LLM simulation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generation model parameters, used when generating samples from a prompt\n",
    "        # template and calibration record. Recommended settings are {\"temperature\": 1.0,\n",
    "        # \"top_p\": 1.0}; additionally maximum tokens will be use case specific.\n",
    "\n",
    "        # Render prompt strings for simulated batch inferencing\n",
    "        rendered_prompts: List[str] = []\n",
    "        for template in templates:\n",
    "            for key, values in template.items():\n",
    "                rendered_prompts.extend(\n",
    "                    self.llm_sim.render_template(template=key, template_values=values)\n",
    "                )\n",
    "\n",
    "        # Simulate LLM batch inferencing\n",
    "        generations = self.llm_sim(\n",
    "            rendered_prompts,\n",
    "            n=1,\n",
    "            prefix=prefix,\n",
    "        )\n",
    "\n",
    "        # Filter out any None values from the generation results and ensure we still\n",
    "        # have a result. Alternatively, we could implement retry logic to handle\n",
    "        # transient LLM errors.\n",
    "        generations_valid = [x for x in generations if x is not None]\n",
    "        if len(generations_valid) == 0:\n",
    "            raise CalibrateException(\"LLM was unable to generate results\")\n",
    "\n",
    "        return generations_valid\n",
    "\n",
    "    def _llm_gradients(self, t: str, mean_score: float, errors: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates gradient instructions to improve the prompt template based on\n",
    "        evaluation feedback.\n",
    "\n",
    "        Args:\n",
    "            t (str): The current prompt template.\n",
    "            mean_score (float): The mean score obtained from the evaluation of the\n",
    "                prompt.\n",
    "            errors (List[str]): A list of error messages or debate history from the\n",
    "                evaluation.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of gradient instructions suggested by the LLM to improve\n",
    "                the prompt template.\n",
    "        \"\"\"\n",
    "\n",
    "        # Build the debate history string from 'errors' returned by the\n",
    "        # evaluate step. Here, we trim the full debate history to the last\n",
    "        # 'grad_debate_rounds' rounds.\n",
    "        debate_turns = errors[-self.grad_debate_rounds * len(self.evaluator.a) :]\n",
    "        debate = \"\\n\".join(debate_turns)\n",
    "\n",
    "        # Compute 'gradients', this is analogous to the back-propagation step when\n",
    "        # training neural networks. I deliberately decided to compute 'g' together in a\n",
    "        # single call to the LLM since gradients must take into account the entire score\n",
    "        # on the entire minibatch.\n",
    "        #\n",
    "        # Insight: the paper did not include the actual response in the gradient\n",
    "        # creation prompt so I left it out as well.\n",
    "\n",
    "        prompt_template = \"\\n\".join(\n",
    "            [\n",
    "                \"I need help improving a text-based prompt template for a language \"\n",
    "                \"model. The current prompt template is:\",\n",
    "                \"\",\n",
    "                \"----- BEGIN PROMPT TEMPLATE -----\",\n",
    "                \"{t}\",\n",
    "                \"----- END PROMPT TEMPLATE -----\",\n",
    "                \"\",\n",
    "                \"The block below contains a debate between AI agents who are deciding how \"\n",
    "                \"to score an output generated by the LLM prompt below:\",\n",
    "                \"\",\n",
    "                \"----- BEGIN DEBATE -----\",\n",
    "                \"{debate}\",\n",
    "                \"----- END DEBATE -----\",\n",
    "                \"\",\n",
    "                \"The agents aligned on an average score of {score} out of 10 after the \"\n",
    "                \"debate.\",\n",
    "                \"\",\n",
    "                \"Your task is to generate a terse and highly specific list of \"\n",
    "                \"instructions to improve the average score by addressing problems \"\n",
    "                \"raised in the debate history.\",\n",
    "                \"\",\n",
    "                \"Your list of instructions may contain up to {grad_depth} \"\n",
    "                \"instructions.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rendered_prompt = self.llm_sim.render_template(\n",
    "            template=prompt_template,\n",
    "            template_values={\n",
    "                \"t\": t,\n",
    "                \"debate\": debate,\n",
    "                \"score\": mean_score,\n",
    "                \"grad_depth\": self.grad_depth,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Call LLM to generate gradients from Evaluate feedback\n",
    "        g = self.llm_sim(\n",
    "            rendered_prompt,\n",
    "            n=1,\n",
    "            prefix=\"GRADIENT\",\n",
    "        )\n",
    "\n",
    "        if g[0] is None:\n",
    "            raise CalibrateException(\"LLM was unable to generate gradients\")\n",
    "\n",
    "        # For simulation purposes, we will expand 'g' to the number of requested\n",
    "        # gradients\n",
    "        derived_g = cast(List[str], [g[0]] * self.grad_depth)\n",
    "\n",
    "        return derived_g\n",
    "\n",
    "    def _llm_sigma(self, t: str, g: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates improved prompt templates by applying gradient instructions to the\n",
    "        current template.\n",
    "\n",
    "        Args:\n",
    "            t (str): The current prompt template.\n",
    "            g (List[str]): A list of gradient instructions to apply.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of improved prompt templates generated by the LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = \"\\n\".join(\n",
    "            [\n",
    "                \"Consider the following natural language prompt template:\",\n",
    "                \"\",\n",
    "                \"----- BEGIN PROMPT TEMPLATE -----\",\n",
    "                \"{t}\",\n",
    "                \"----- END PROMPT TEMPLATE -----\",\n",
    "                \"\",\n",
    "                \"Your task is to apply the following suggestions to improve the template:\",\n",
    "                \"{g}\",\n",
    "                \"\",\n",
    "                \"Note that the prompt template contains Python f-string style substitution \"\n",
    "                \"placeholders enclosed in curly braces, your improved prompt must preserve \"\n",
    "                \"all template placeholders.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rendered_prompt = self.llm_sim.render_template(\n",
    "            template=prompt_template,\n",
    "            template_values={\n",
    "                \"t\": t,\n",
    "                \"g\": g,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # For the simulation, we will extract the template fields from the original\n",
    "        # prompt template and just append to the end of the generation so the output will\n",
    "        # pass the check below\n",
    "        template_keys = set(re.findall(r\"{(.*?)}\", t))\n",
    "        template_key_str = \" \".join([f\"{{{x}}}\" for x in template_keys])\n",
    "\n",
    "        # Update the original prompt template with the 'gradients'. In the\n",
    "        # paper there is an additional step to generate additional monte-carlo\n",
    "        # successors from p', in this implementation we accomplish this by\n",
    "        # increasing the number of generations requested from the LLM.\n",
    "        t_expanded = self.llm_sim(\n",
    "            rendered_prompt,\n",
    "            n=self.mc_successors,\n",
    "            prefix=\"EXPANDED\",\n",
    "            append=template_key_str,\n",
    "        )\n",
    "\n",
    "        # Ensure that the expanded templates contain all the original template keys.\n",
    "        template_patterns = [f\"{{{x}}}\" for x in template_keys]\n",
    "        t_expand_validated: List[str] = []\n",
    "        for t_exp in t_expanded:\n",
    "            if t_exp is None:\n",
    "                continue\n",
    "\n",
    "            # Verify that all patterns in template_patterns are in t_exp\n",
    "            if not all(re.search(pattern, t_exp) for pattern in template_patterns):\n",
    "                LOG.warning(\n",
    "                    \"Expanded prompt does not contain all original template keys\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            t_expand_validated.append(t_exp)\n",
    "\n",
    "        # If the validated list of expanded prompts is less than the requested number of\n",
    "        # monte-carlo successors, raise an exception. Alternatively a threshold can be\n",
    "        # checked for fault tolerance.\n",
    "        if len(t_expand_validated) < self.mc_successors:\n",
    "            raise CalibrateException(\n",
    "                f\"Only {len(t_expand_validated)} valid expanded prompts generated; expected {self.mc_successors}\"\n",
    "            )\n",
    "\n",
    "        return t_expand_validated\n",
    "\n",
    "    def _score(\n",
    "        self, *, batch: List[Tuple[str, List[str]]]\n",
    "    ) -> List[Tuple[float, List[str]] | None]:\n",
    "        \"\"\"\n",
    "        Computes evaluation scores for a batch of prompt templates and their generated\n",
    "        outputs.\n",
    "\n",
    "        Args:\n",
    "            batch (List[Tuple[str, List[str]]]): A list of tuples where each tuple\n",
    "                contains a prompt template (str) and a list of generated outputs\n",
    "                (List[str]) for that template.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[float, List[str]] | None]: A list where each item is either a\n",
    "                tuple containing the mean score (float) and a list of errors\n",
    "                (List[str]) for each prompt, or None if evaluation failed.\n",
    "        \"\"\"\n",
    "\n",
    "        eval_results = self.evaluator.evaluate_batch(batch=batch)\n",
    "\n",
    "        scores: List[Tuple[float, List[str]] | None] = [None] * len(batch)\n",
    "        for i, cur_eval_result in enumerate(eval_results):\n",
    "            if cur_eval_result is None:\n",
    "                continue\n",
    "\n",
    "            S, errors = cur_eval_result\n",
    "\n",
    "            score = self.evaluator.compute_normalized_scores(S)\n",
    "            mean_score = np.mean(score)\n",
    "\n",
    "            scores[i] = (mean_score, errors)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _expand(self, t: str, d_cal: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Expands the given prompt template by generating multiple improved templates\n",
    "          using calibration data.\n",
    "\n",
    "        Args:\n",
    "            t (str): The prompt template to be expanded.\n",
    "            d_cal (List[Dict[str, Any]]): The calibration data used for expanding the\n",
    "                prompt.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of expanded prompt templates generated from the original\n",
    "                template.\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample a minibatch of calibration data\n",
    "        minibatch = random.sample(d_cal, self.minibatch_exp_len)\n",
    "\n",
    "        # For the current beam, generate responses from the LLM\n",
    "        generations = self._llm_generate(\n",
    "            templates=[{t: minibatch}], prefix=\"GEN_EXPAND\"\n",
    "        )\n",
    "\n",
    "        if len(generations) != len(minibatch):\n",
    "            raise ValueError(\n",
    "                \"Error generating prompt template expansions. Number of generations \"\n",
    "                \"does not match the minibatch size.\"\n",
    "            )\n",
    "\n",
    "        # Compute the 'loss' (score) for the expanded prompt template generations\n",
    "        scores = self._score(batch=[(t, generations)])\n",
    "\n",
    "        if len(scores) != 1 or scores[0] is None:\n",
    "            raise ValueError(\"Error scoring prompt generation during expand\")\n",
    "\n",
    "        # pylint: disable=unpacking-non-sequence\n",
    "        mean_score, errors = scores[0]\n",
    "\n",
    "        # Generate gradients from the evaluation feedback by investigating what went\n",
    "        # wrong given the debate history (errors).\n",
    "        g = self._llm_gradients(t=t, mean_score=mean_score, errors=errors)\n",
    "\n",
    "        t_expanded = self._llm_sigma(t=t, g=g)\n",
    "\n",
    "        return t_expanded\n",
    "\n",
    "    def _select_ucb(self, C: Set[str], d_cal: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        raise NotImplementedError(\"UCB selection not implemented yet\")\n",
    "\n",
    "    def _select_fast(\n",
    "        self, C: Set[str], d_cal: List[Dict[str, Any]]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Selects the best prompt templates from candidates using the UCB\n",
    "        (Upper Confidence Bound) method.\n",
    "\n",
    "        Args:\n",
    "            C (Set[str]): A set of candidate prompt templates.\n",
    "            d_cal (List[Dict[str, Any]]): The calibration data for scoring the\n",
    "                candidates.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary mapping prompt templates to their\n",
    "                evaluation scores.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method is not implemented yet.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert C into an ordered list so we can properly re-assemble the order of\n",
    "        # batch inference generations\n",
    "        C_ordered = list(C)\n",
    "\n",
    "        # Build generation units of work for batch generation\n",
    "        gen_batch = []\n",
    "        for c in C_ordered:\n",
    "            minibatch = random.sample(d_cal, self.minibatch_sel_len)\n",
    "            gen_batch.append({c: minibatch})\n",
    "\n",
    "        generations = self._llm_generate(templates=gen_batch, prefix=\"GEN_SELECT\")\n",
    "\n",
    "        # Match the batch generations to the original prompt template\n",
    "        score_batch: List[Tuple[str, List[str]]] = []\n",
    "        for generation_index, generation in enumerate(generations):\n",
    "\n",
    "            score_batch_index = generation_index // self.minibatch_sel_len\n",
    "\n",
    "            if generation_index % self.minibatch_sel_len == 0:\n",
    "                score_batch.append((C_ordered[score_batch_index], []))\n",
    "\n",
    "            score_batch[score_batch_index][1].append(generation)\n",
    "\n",
    "        scores = self._score(batch=score_batch)\n",
    "\n",
    "        # Flatten the scores list, note that errors returned by the scoring process (if\n",
    "        # any) are re-mapped to 0.0 and should be filtered out when we build the new\n",
    "        # beam below. Note that scores are returned in the same order as the list of\n",
    "        # inputs. Finally we are dropping the gradients since they are not being used\n",
    "        # during selection.\n",
    "        scores_flat = [x[0] if x is not None else 0.0 for x in scores]\n",
    "\n",
    "        # Re-assemble the scored components into the data structure that represents our\n",
    "        # selected beam (B_prime). An important design element here is that we are not\n",
    "        # filtering out any scores since the outer calibration method will combine\n",
    "        # B_prime with the current beam (B) and then select the best scores to ensure\n",
    "        # the the highest scores globally are kept in the search.\n",
    "        B_prime = dict(\n",
    "            zip(\n",
    "                [x[0] for x in score_batch],\n",
    "                scores_flat,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return B_prime\n",
    "\n",
    "    def calibrate(self, t_0: str, d_cal: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Executes the TextEvolve Calibrate algorithm to calibrate an initial prompt\n",
    "        template against a set of calibration template values.\n",
    "\n",
    "        Args:\n",
    "            t_0 (str): The initial prompt template containing placeholders for the\n",
    "                calibration dataset.\n",
    "            d_cal (List[Dict[str, Any]]): The calibration dataset containing template\n",
    "                parameters.\n",
    "\n",
    "        Returns:\n",
    "            str: The highest scoring prompt template after calibration.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize beam B_0 with our un-scored initial template\n",
    "        B: Dict[str, float] = {t_0: 0.0}\n",
    "\n",
    "        # Iterate over the search depth (r rounds)\n",
    "        for i in range(self.r):\n",
    "\n",
    "            # C is a dictionary of expanded candidates for this round\n",
    "            C: Set[str] = set()\n",
    "\n",
    "            # Expand each beam candidate, note for logging here we add 0 fill since it\n",
    "            # is in a loop and will be easier to read if the message lengths line up.\n",
    "            for B_index, t in enumerate(B):\n",
    "                t_prime = self._expand(t=t, d_cal=d_cal)\n",
    "                C.update(t_prime)\n",
    "                LOG.info(\n",
    "                    \"r=%02i expand(B=%02i): len(t')=%02i, len(C)=%02i\",\n",
    "                    i + 1,\n",
    "                    B_index,\n",
    "                    len(t_prime),\n",
    "                    len(C),\n",
    "                )\n",
    "\n",
    "            # Select - there is a deep-learning analogy here with validation.\n",
    "            if self.select_method.lower() == \"fast\":\n",
    "\n",
    "                B_prime = self._select_fast(C=C, d_cal=d_cal)\n",
    "\n",
    "            elif self.select_method.lower() == \"ucb\":\n",
    "                B_prime = self._select_ucb(C=C, d_cal=d_cal)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid selection method: {self.select_method}\")\n",
    "\n",
    "            # Insight - common operations of the select step happen below.\n",
    "\n",
    "            # Combine the current beams (B) with the selected beams from this round\n",
    "            # (B_prime) and select the scores up to the configured beam width.\n",
    "            B_new = Calibrate._top_k_beams(\n",
    "                beams={\n",
    "                    **B,\n",
    "                    **B_prime,\n",
    "                },\n",
    "                k=self.b,\n",
    "            )\n",
    "\n",
    "            # Count the number of keys from B_new that are not in B; basically this will\n",
    "            # tell use how many new prompt templates were discovered in this round. This\n",
    "            # will provide insight into how the algorithm is improving the prompt\n",
    "            # template and also give data needed to trigger early stopping.\n",
    "            new_beams = len(set(B_new.keys()) - set(B.keys()))\n",
    "\n",
    "            B = B_new\n",
    "\n",
    "            # Generate a helpful log message to give calibration status for each round\n",
    "            LOG.info(\n",
    "                \"r=%02i select[%s]: len(B')=%i, new beams=%i, len(B)=%i, B mean=%.4f (%s)\",\n",
    "                i + 1,\n",
    "                self.select_method.lower(),\n",
    "                len(B_prime),\n",
    "                new_beams,\n",
    "                len(B),\n",
    "                np.array(list(B.values())).mean(),\n",
    "                \", \".join(f\"{val:.4f}\" for val in B.values()),\n",
    "            )\n",
    "\n",
    "            if self.early_stopping and new_beams == 0:\n",
    "                LOG.info(\"Early stopping triggered, no new beams discovered\")\n",
    "                break\n",
    "\n",
    "        # Select the top beam from the final round\n",
    "        best_candidate = max(B, key=B.get)  # type: ignore\n",
    "\n",
    "        return best_candidate\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293ffca-40b2-4c82-8bf9-dc67da42963b",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609912cb-fcb8-43bc-afd9-a1b561d69d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 14:15:38,575 - INFO - r=01 expand(B=00): len(t')=05, len(C)=05\n",
      "2024-09-19 14:15:38,595 - INFO - r=01 select[fast]: len(B')=5, new beams=5, len(B)=6, B mean=4.5985 (0.0000, 5.4957, 5.5326, 5.5649, 5.4375, 5.5604)\n",
      "2024-09-19 14:15:38,600 - INFO - r=02 expand(B=00): len(t')=05, len(C)=05\n",
      "2024-09-19 14:15:38,605 - INFO - r=02 expand(B=01): len(t')=05, len(C)=10\n",
      "2024-09-19 14:15:38,610 - INFO - r=02 expand(B=02): len(t')=05, len(C)=15\n",
      "2024-09-19 14:15:38,616 - INFO - r=02 expand(B=03): len(t')=05, len(C)=20\n",
      "2024-09-19 14:15:38,622 - INFO - r=02 expand(B=04): len(t')=05, len(C)=25\n",
      "2024-09-19 14:15:38,627 - INFO - r=02 expand(B=05): len(t')=05, len(C)=30\n",
      "2024-09-19 14:15:38,744 - INFO - r=02 select[fast]: len(B')=30, new beams=8, len(B)=9, B mean=5.5993 (5.5610, 5.5628, 5.5649, 5.5711, 5.5901, 5.6002, 5.6396, 5.6456, 5.6583)\n",
      "2024-09-19 14:15:38,749 - INFO - r=03 expand(B=00): len(t')=05, len(C)=05\n",
      "2024-09-19 14:15:38,754 - INFO - r=03 expand(B=01): len(t')=05, len(C)=10\n",
      "2024-09-19 14:15:38,759 - INFO - r=03 expand(B=02): len(t')=05, len(C)=15\n",
      "2024-09-19 14:15:38,764 - INFO - r=03 expand(B=03): len(t')=05, len(C)=20\n",
      "2024-09-19 14:15:38,770 - INFO - r=03 expand(B=04): len(t')=05, len(C)=25\n",
      "2024-09-19 14:15:38,776 - INFO - r=03 expand(B=05): len(t')=05, len(C)=30\n",
      "2024-09-19 14:15:38,780 - INFO - r=03 expand(B=06): len(t')=05, len(C)=35\n",
      "2024-09-19 14:15:38,785 - INFO - r=03 expand(B=07): len(t')=05, len(C)=40\n",
      "2024-09-19 14:15:38,790 - INFO - r=03 expand(B=08): len(t')=05, len(C)=45\n",
      "2024-09-19 14:15:39,008 - INFO - r=03 select[fast]: len(B')=45, new beams=5, len(B)=9, B mean=5.6496 (5.6002, 5.6079, 5.6114, 5.6396, 5.6456, 5.6583, 5.6725, 5.6739, 5.7368)\n",
      "2024-09-19 14:15:39,014 - INFO - r=04 expand(B=00): len(t')=05, len(C)=05\n",
      "2024-09-19 14:15:39,020 - INFO - r=04 expand(B=01): len(t')=05, len(C)=10\n",
      "2024-09-19 14:15:39,025 - INFO - r=04 expand(B=02): len(t')=05, len(C)=15\n",
      "2024-09-19 14:15:39,031 - INFO - r=04 expand(B=03): len(t')=05, len(C)=20\n",
      "2024-09-19 14:15:39,035 - INFO - r=04 expand(B=04): len(t')=05, len(C)=25\n",
      "2024-09-19 14:15:39,041 - INFO - r=04 expand(B=05): len(t')=05, len(C)=30\n",
      "2024-09-19 14:15:39,046 - INFO - r=04 expand(B=06): len(t')=05, len(C)=35\n",
      "2024-09-19 14:15:39,052 - INFO - r=04 expand(B=07): len(t')=05, len(C)=40\n",
      "2024-09-19 14:15:39,058 - INFO - r=04 expand(B=08): len(t')=05, len(C)=45\n",
      "2024-09-19 14:15:39,248 - INFO - r=04 select[fast]: len(B')=45, new beams=3, len(B)=9, B mean=5.6715 (5.6396, 5.6456, 5.6490, 5.6583, 5.6725, 5.6739, 5.6750, 5.6926, 5.7368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo optimized prompt: EXPANDED,f6fe2dd0969e4c83847509d6dd373415,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA,{topic} {document}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Customize the log format\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Stream the logs to the notebook's output\n",
    "    ]\n",
    ")\n",
    "\n",
    "b = 9\n",
    "r = 4\n",
    "minibatch_exp_len = 5\n",
    "minibatch_sel_len = 6\n",
    "grad_depth = 10\n",
    "grad_debate_rounds = 2\n",
    "mc_successors = 5\n",
    "\n",
    "# Configure the TextEvolve Evaluate function. Here, we are simulating a multi-agent debate among \n",
    "# 3 agents: the critic, the supporter, and the neutral observer \n",
    "evaluator = Evaluate(\n",
    "    a=[\"Critic\", \"Supporter\", \"Neutral Observer\"],\n",
    "    w=np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
    "    r=2,\n",
    "    c=0.05,\n",
    "    l=1,\n",
    ")\n",
    "\n",
    "prompt_calibrator = Calibrate(\n",
    "    b=b,\n",
    "    r=r,\n",
    "    evaluator=evaluator,\n",
    "    minibatch_exp_len=minibatch_exp_len,\n",
    "    minibatch_sel_len=minibatch_sel_len,\n",
    "    grad_depth=grad_depth,\n",
    "    grad_debate_rounds=grad_debate_rounds,\n",
    "    mc_successors=mc_successors,\n",
    "    select_method=\"fast\",\n",
    ")\n",
    "\n",
    "d_cal = [\n",
    "    {\"document\": \"report\", \"topic\": \"AI\"},\n",
    "    {\"document\": \"essay\", \"topic\": \"philosophy\"},\n",
    "    {\"document\": \"article\", \"topic\": \"technology\"},\n",
    "    {\"document\": \"post\", \"topic\": \"economics\"},\n",
    "    {\"document\": \"review\", \"topic\": \"politics\"},\n",
    "    {\"document\": \"paper\", \"topic\": \"history\"},\n",
    "    {\"document\": \"guide\", \"topic\": \"science\"},\n",
    "    {\"document\": \"letter\", \"topic\": \"art\"},\n",
    "    {\"document\": \"memo\", \"topic\": \"education\"},\n",
    "    {\"document\": \"summary\", \"topic\": \"health\"},\n",
    "    {\"document\": \"proposal\", \"topic\": \"business\"},\n",
    "    {\"document\": \"manual\", \"topic\": \"literature\"},\n",
    "    {\"document\": \"story\", \"topic\": \"environment\"},\n",
    "    {\"document\": \"script\", \"topic\": \"culture\"},\n",
    "    {\"document\": \"novel\", \"topic\": \"society\"},\n",
    "    {\"document\": \"blog\", \"topic\": \"media\"},\n",
    "    {\"document\": \"journal\", \"topic\": \"sports\"},\n",
    "    {\"document\": \"thesis\", \"topic\": \"entertainment\"},\n",
    "    {\"document\": \"dissertation\", \"topic\": \"food\"},\n",
    "    {\"document\": \"book\", \"topic\": \"travel\"},\n",
    "]\n",
    "\n",
    "t_0 = \"Write a {document} about {topic}\"\n",
    "\n",
    "optimized_prompt = prompt_calibrator.calibrate(\n",
    "    t_0=t_0,\n",
    "    d_cal=d_cal\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Demo optimized prompt: {optimized_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7f353-3091-40e7-abb4-c5bae4058030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
