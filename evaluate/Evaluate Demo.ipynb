{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a3d7bc-6d3c-4d28-8da3-89f1e98b3a7f",
   "metadata": {},
   "source": [
    "# TextEvolve Evaluate\n",
    "\n",
    "This notebook contains a demonstration implementation of the TextEvolve Evaluate service, inspired by ChatEval ([arXiv:2308.07201](https://arxiv.org/abs/2308.07201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dbda6a-d2a7-4bd9-9584-e2f4c17de011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class TextEvolveEvaluator:\n",
    "    def __init__(self, a: List[str], w: np.ndarray, r: int, c: float, l: int):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the given configuration settings.\n",
    "\n",
    "        Args:\n",
    "            a (List[str]): List of debater agents.\n",
    "            w (np.ndarray): Weight vector for score components.\n",
    "            r (int): Number of debate rounds.\n",
    "            c (float): Convergence threshold for early stopping.\n",
    "            l (int): Debate history parameter (currently a placeholder).\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.w = w\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.l = l  # Placeholder for the debate history length\n",
    "\n",
    "    @staticmethod\n",
    "    def llm(x: str, y: List[str], xi: List[str], m_i: List[str], j: int, k: int, round_num: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulate the LLM scoring process for an agent using a truncated normal distribution.\n",
    "        This simulation controls the coefficient of variation (CV) to mimic agents reaching consensus over time.\n",
    "\n",
    "        Args:\n",
    "            x (str): Input context in natural language.\n",
    "            y (List[str]): List of candidate responses.\n",
    "            xi (List[str]): Debate history up to the current round.\n",
    "            m_i (List[str]): Memories specific to the current agent.\n",
    "            j (int): Number of candidate responses.\n",
    "            k (int): Number of score components.\n",
    "            round_num (int): The current round number.\n",
    "            initial_cv (float): Initial coefficient of variation for the first round.\n",
    "            cv_decrease_percent (float): Percentage by which CV decreases each round.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A j x k matrix with scores between 0.0 and 10.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Note: This implementation of LLM is simulating scores and not part of the core algorithm. \n",
    "        \n",
    "        # Simulation configuration: The Initial coefficient of variation for the first round.\n",
    "        initial_cv: float = 0.2\n",
    "\n",
    "        # Simulation configuration: Percentage by which CV decreases each round.\n",
    "        cv_decrease_percent: float = 10.0\n",
    "\n",
    "        # Generate mean scores for each candidate response and score component\n",
    "        # The mean is randomly chosen between 4.5 and 5.5 to center the scores around the middle of the 0-10 range\n",
    "        mean_score = np.random.uniform(4.5, 5.5, (j, k))  # Shape: (j, k)\n",
    "\n",
    "        # Compute the current coefficient of variation (CV) for this round\n",
    "        # CV decreases with each round by the specified percentage to simulate agents reaching consensus\n",
    "        current_cv = initial_cv * (1 - cv_decrease_percent / 100.0) ** round_num\n",
    "\n",
    "        # Standard deviation is calculated as a proportion of the mean score, based on the current CV\n",
    "        std_dev = mean_score * current_cv  # Shape: (j, k)\n",
    "\n",
    "        # Set the lower and upper bounds for the scores to ensure they stay within the valid range [0.0, 10.0]\n",
    "        lower, upper = 0.0, 10.0\n",
    "\n",
    "        # Generate scores using a truncated normal distribution\n",
    "        # The scores are generated such that they fall within the [0.0, 10.0] range, following a normal distribution centered on mean_score with std_dev\n",
    "        scores = truncnorm(\n",
    "            (lower - mean_score) / std_dev,  # Lower bound in standardized units\n",
    "            (upper - mean_score) / std_dev,  # Upper bound in standardized units\n",
    "            loc=mean_score,  # Mean of the distribution\n",
    "            scale=std_dev   # Standard deviation of the distribution\n",
    "        ).rvs()  # Generate random variates\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cv(matrix: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the coefficient of variation (CV) for a given score matrix.\n",
    "\n",
    "        Args:\n",
    "            matrix (np.ndarray): A matrix of scores (i x j x k) from which CV is computed.\n",
    "\n",
    "        Returns:\n",
    "            float: The coefficient of variation of the scores.\n",
    "        \"\"\"\n",
    "        return np.std(matrix) / np.mean(matrix)\n",
    "\n",
    "    def evaluate(self, x: str, y: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Evaluate the candidate responses using the TextEvolve evaluation function.\n",
    "\n",
    "        Args:\n",
    "            x (str): Input context in natural language.\n",
    "            y (List[str]): List of candidate responses.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The scoring tensor S with dimensions (r x i x j x k).\n",
    "        \"\"\"\n",
    "        j = len(y)  # Number of candidate responses\n",
    "        k = len(self.w)  # Number of score components\n",
    "        i = len(self.a)  # Number of agents\n",
    "        S = np.zeros((self.r, i, j, k))  # Initialize scoring tensor\n",
    "\n",
    "        for round_num in range(self.r):\n",
    "            for agent_num in range(i):\n",
    "                # Placeholder for debate history\n",
    "                xi = []\n",
    "\n",
    "                # This is placeholder for agent memories, in a real implementation, xi and m_i would be \n",
    "                # derived from actual debate history and x y values\n",
    "                m_i = [] \n",
    "                \n",
    "                S[round_num, agent_num] = self.llm(x, y, xi, m_i, j, k, round_num)\n",
    "\n",
    "            # After each round, compute CV and check for early stopping\n",
    "            round_cv = self.compute_cv(S[round_num])\n",
    "            print(f\"Round {round_num + 1} CV: {round_cv:.4f}\")\n",
    "            print(f\"Scores after Round {round_num + 1}:\\n{S[round_num]}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if round_cv <= self.c:\n",
    "                print(f\"Early stopping triggered at Round {round_num + 1} (CV <= {self.c})\")\n",
    "                S = S[:round_num + 1]  # Truncate the tensor to the completed rounds\n",
    "                break\n",
    "\n",
    "        return S\n",
    "\n",
    "    def compute_normalized_scores(self, S: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the normalized scores for each response candidate.\n",
    "\n",
    "        Args:\n",
    "            S (np.ndarray): The scoring tensor with dimensions (r x i x j x k).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Normalized score vector for each candidate response.\n",
    "        \"\"\"\n",
    "        return np.sum(S * self.w, axis=(0, 1, 3)) / np.prod(S.shape[:-2])\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_softmax_scores(s_norm: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the softmax scores for each response candidate.\n",
    "\n",
    "        Args:\n",
    "            s_norm (np.ndarray): Normalized score vector.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Softmax score vector representing probabilities.\n",
    "        \"\"\"\n",
    "        e_x = np.exp(s_norm - np.max(s_norm))  # Subtract max for numerical stability\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def select_best_candidate(self, S: np.ndarray, y: List[str], probabilistic: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Select the best response candidate based on normalized or softmax scores.\n",
    "\n",
    "        Args:\n",
    "            S (np.ndarray): The scoring tensor with dimensions (r x i x j x k).\n",
    "            y (List[str]): List of candidate responses.\n",
    "            probabilistic (bool): If True, selects based on softmax scores; otherwise, based on normalized scores.\n",
    "\n",
    "        Returns:\n",
    "            str: The selected response candidate.\n",
    "        \"\"\"\n",
    "        # Compute normalized scores\n",
    "        s_norm = self.compute_normalized_scores(S)\n",
    "        \n",
    "        if probabilistic:\n",
    "            # Compute softmax scores\n",
    "            s_phi = self.compute_softmax_scores(s_norm)\n",
    "            print(f\"Softmax Scores:\\n{s_phi}\")\n",
    "            # Select the best response candidate probabilistically using softmax scores\n",
    "            best_index = np.random.choice(np.arange(len(y)), p=s_phi)\n",
    "        else:\n",
    "            print(f\"Normalized Scores:\\n{s_norm}\")\n",
    "            # Select the best response candidate using normalized scores\n",
    "            best_index = np.argmax(s_norm)\n",
    "\n",
    "        print(f\"Best candidate: {y[best_index]}\")\n",
    "        return y[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079cade-0722-4a0f-9140-68f118e6befa",
   "metadata": {},
   "source": [
    "## Run 1: All Debate Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1587be11-820b-4ac4-b3e1-fdc5605a5bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 CV: 0.1910\n",
      "Scores after Round 1:\n",
      "[[[5.56551277 6.47592468 5.20177472 5.54886533 5.34811008 5.77335109]\n",
      "  [3.79044705 6.43762504 4.24772305 5.24323777 5.04434372 3.11961379]\n",
      "  [5.27975442 5.070902   5.86902891 5.68094046 4.07851425 5.32363434]\n",
      "  [5.16997582 5.13113557 3.16900574 3.35050658 5.33962937 5.25523733]]\n",
      "\n",
      " [[4.25917555 5.42478793 4.53769133 5.05544246 4.76753478 6.54091629]\n",
      "  [4.61246683 4.84761556 5.2237192  7.47980295 4.5297462  4.5268935 ]\n",
      "  [4.67085727 6.22199493 5.42572746 4.22478607 5.18227605 4.81687637]\n",
      "  [4.11884819 5.00492179 4.43466791 4.99063672 6.22687015 6.93851698]]\n",
      "\n",
      " [[4.05193675 3.99576715 4.78230353 3.76518525 5.35236348 5.62661895]\n",
      "  [4.55960648 6.66019104 5.41190266 4.76137714 2.53464633 5.29412747]\n",
      "  [4.96274897 5.50484233 4.75056531 4.60678359 6.0066978  3.44586018]\n",
      "  [3.98700427 7.31215325 4.29068005 3.81483393 4.83808499 4.04373613]]]\n",
      "--------------------------------------------------\n",
      "Round 2 CV: 0.1915\n",
      "Scores after Round 2:\n",
      "[[[4.66574882 6.5646331  4.92594313 4.9383969  5.07501109 3.79474336]\n",
      "  [6.27132307 5.41964594 2.80220861 5.80672594 5.39161055 5.02260502]\n",
      "  [5.88897573 6.24668125 4.8176127  5.07318446 6.30758492 5.46334107]\n",
      "  [5.05442943 7.0327223  4.29329062 5.29078946 4.37996151 3.88868913]]\n",
      "\n",
      " [[5.99017873 5.24680955 3.95032125 4.83975801 3.83535322 5.74959796]\n",
      "  [3.50092045 3.15716587 4.9703518  3.55189464 5.23281543 5.62367127]\n",
      "  [4.71010736 4.84348044 3.68995389 6.38826987 4.96187446 4.82293864]\n",
      "  [6.16929214 2.06596374 5.00103564 4.26332466 5.33596139 5.2833446 ]]\n",
      "\n",
      " [[5.47118347 5.08236986 4.3060899  6.5064346  5.66174647 4.52416343]\n",
      "  [6.93257831 6.02911    5.73039842 4.27037616 4.83832522 3.5695578 ]\n",
      "  [4.61743782 3.89859905 4.83424042 5.08959889 4.83033526 4.41306519]\n",
      "  [5.33172356 6.60104674 5.7948816  5.7180648  4.37331819 5.10028274]]]\n",
      "--------------------------------------------------\n",
      "Normalized Scores:\n",
      "[35.99867288 34.83295396 35.63442869 35.25708507]\n",
      "Best candidate: Paris\n",
      "Softmax Scores:\n",
      "[0.4027748  0.1255443  0.27981633 0.19186458]\n",
      "Best candidate: Marseille\n"
     ]
    }
   ],
   "source": [
    "a = [\"Critic\", \"Supporter\", \"Neutral Observer\"]\n",
    "w = np.array([1.0, 2.0, 1.0, 1.0, 1.0, 1.0])\n",
    "r = 2\n",
    "c = 0.05\n",
    "l = 3\n",
    "\n",
    "evaluator = TextEvolveEvaluator(a, w, r, c, l)\n",
    "\n",
    "x = \"What is the capital of France?\"\n",
    "y = [\"Paris\", \"Lyon\", \"Marseille\", \"Bordeaux\"]\n",
    "\n",
    "S = evaluator.evaluate(x, y)\n",
    "best_norm = evaluator.select_best_candidate(S, y, probabilistic=False)\n",
    "best_phi = evaluator.select_best_candidate(S, y, probabilistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bc425-1ed3-4696-a691-1ac20f733d51",
   "metadata": {},
   "source": [
    "## Run 2: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f277e9b-12f6-4792-9c07-5210d2b14b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 CV: 0.2061\n",
      "Scores after Round 1:\n",
      "[[[4.40003482 5.76765998 4.41084156 4.31928368 5.83284199 4.74998494]\n",
      "  [4.62425354 5.36633232 4.95296737 3.87591941 5.96994978 6.66678477]\n",
      "  [4.20660256 4.92685516 4.3238116  4.69895344 6.06101375 2.71496257]\n",
      "  [7.02305673 4.58487347 7.37838062 6.67655097 4.52597735 4.46062579]]\n",
      "\n",
      " [[3.95553449 3.7668584  6.0774995  3.44993817 5.47137199 4.28423879]\n",
      "  [4.73745441 5.5172704  4.8420824  7.85598858 4.14153904 5.40155101]\n",
      "  [4.71989264 7.0849561  7.09259026 6.47248176 4.98430326 5.93845128]\n",
      "  [5.49111395 5.87566403 4.00080663 4.59489754 4.90853868 6.30478466]]\n",
      "\n",
      " [[4.19929269 6.29161438 7.37663719 5.70719008 4.7922499  5.32176347]\n",
      "  [6.04777018 5.89642721 3.48786916 5.47774224 4.04378048 6.5586131 ]\n",
      "  [5.99654923 4.51092812 5.48205854 6.01867858 4.7433706  4.98619432]\n",
      "  [5.67946141 6.2573098  3.91824824 6.47105409 4.7091243  3.35459124]]]\n",
      "--------------------------------------------------\n",
      "Round 2 CV: 0.1935\n",
      "Scores after Round 2:\n",
      "[[[4.13786967 7.91553875 6.37793762 7.1334223  5.2426347  5.09226824]\n",
      "  [3.85374458 5.62070208 4.95488149 5.82699327 4.681575   4.87044809]\n",
      "  [5.84243748 3.82438    4.93900764 5.06736853 3.15111367 4.04214043]\n",
      "  [5.35977433 4.60962048 4.18922086 4.09664557 3.79861615 6.11819066]]\n",
      "\n",
      " [[4.96455462 5.1516452  6.52763304 4.56516837 6.23202144 6.11189766]\n",
      "  [3.82862368 6.7391899  4.37787912 5.61269557 3.55783573 4.50538276]\n",
      "  [4.02748488 4.94899652 4.75304116 4.34848077 5.67417562 4.73814486]\n",
      "  [5.10436959 3.81737238 5.21922211 3.84357365 5.95965754 4.92459539]]\n",
      "\n",
      " [[4.96879769 5.1771844  4.13116943 3.88279825 4.41424086 6.04050568]\n",
      "  [3.65096118 4.61910638 5.38903572 2.93386432 5.05507531 5.67444761]\n",
      "  [5.04269264 4.52234806 5.05085408 6.74812706 3.69185643 4.23065522]\n",
      "  [4.43642002 6.40202017 5.09732804 5.79147358 5.2234113  5.4943985 ]]]\n",
      "--------------------------------------------------\n",
      "Early stopping triggered at Round 2 (CV <= 0.2)\n",
      "Normalized Scores:\n",
      "[37.05210418 35.82929424 34.90407046 36.2079717 ]\n",
      "Best candidate: Paris\n",
      "Softmax Scores:\n",
      "[0.54316967 0.1599101  0.06339519 0.23352503]\n",
      "Best candidate: Paris\n"
     ]
    }
   ],
   "source": [
    "a = [\"Critic\", \"Supporter\", \"Neutral Observer\"]\n",
    "w = np.array([1.0, 2.0, 1.0, 1.0, 1.0, 1.0])\n",
    "r = 2\n",
    "c = 0.2\n",
    "l = 3\n",
    "\n",
    "evaluator = TextEvolveEvaluator(a, w, r, c, l)\n",
    "\n",
    "x = \"What is the capital of France?\"\n",
    "y = [\"Paris\", \"Lyon\", \"Marseille\", \"Bordeaux\"]\n",
    "\n",
    "S = evaluator.evaluate(x, y)\n",
    "best_norm = evaluator.select_best_candidate(S, y, probabilistic=False)\n",
    "best_phi = evaluator.select_best_candidate(S, y, probabilistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef7b41-93df-47ef-87f3-743a158398bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
